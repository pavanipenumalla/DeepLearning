{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.sources\") as f:\n",
    "    train_sources = f.readlines()\n",
    "with open(\"train.targets\") as f:\n",
    "    train_targets = f.readlines()\n",
    "\n",
    "train_sources_tokenized = []\n",
    "for i in range(len(train_sources)):\n",
    "    train_sources_tokenized.append(train_sources[i].replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "train_targets_tokenized = []\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets_tokenized.append(train_targets[i].replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "train_sources_vocabulary = []\n",
    "for i in range(len(train_sources_tokenized)):\n",
    "    for j in range(len(train_sources_tokenized[i])):\n",
    "        if train_sources_tokenized[i][j] not in train_sources_vocabulary:\n",
    "            train_sources_vocabulary.append(train_sources_tokenized[i][j])\n",
    "            \n",
    "train_targets_vocabulary = []\n",
    "for i in range(len(train_targets_tokenized)):\n",
    "    for j in range(len(train_targets_tokenized[i])):\n",
    "        if train_targets_tokenized[i][j] not in train_targets_vocabulary:\n",
    "            train_targets_vocabulary.append(train_targets_tokenized[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500  \n",
    "train_sources_integer_sequences_padded = np.zeros((len(train_sources_tokenized), MAX_SEQUENCE_LENGTH))\n",
    "train_targets_integer_sequences_padded = np.zeros((len(train_targets_tokenized), MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "for i in range(len(train_sources_tokenized)):\n",
    "    for j in range(min(MAX_SEQUENCE_LENGTH, len(train_sources_tokenized[i]))):\n",
    "        token = train_sources_tokenized[i][j]\n",
    "        if token in train_sources_vocabulary:\n",
    "            train_sources_integer_sequences_padded[i, j] = train_sources_vocabulary.index(token)\n",
    "        else:\n",
    "            train_sources_integer_sequences_padded[i, j] = train_sources_vocabulary.index('<OOV>')\n",
    "\n",
    "for i in range(len(train_targets_tokenized)):\n",
    "    for j in range(min(MAX_SEQUENCE_LENGTH, len(train_targets_tokenized[i]))):\n",
    "        token = train_targets_tokenized[i][j]\n",
    "        if token in train_targets_vocabulary:\n",
    "            train_targets_integer_sequences_padded[i, j] = train_targets_vocabulary.index(token)\n",
    "        else:\n",
    "            train_targets_integer_sequences_padded[i, j] = train_targets_vocabulary.index('<OOV>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(train_sources_integer_sequences_padded).long()\n",
    "Y = torch.from_numpy(train_targets_integer_sequences_padded).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172719, 500])\n",
      "torch.Size([172719, 500])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_data = TensorDataset(X_train, Y_train)\n",
    "test_data = TensorDataset(X_test, Y_test)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        hidden = (hidden[::2, :, :] + hidden[1::2, :, :]) / 2\n",
    "        cell = (cell[::2, :, :] + cell[1::2, :, :]) / 2\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = torch.nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.v = torch.nn.Parameter(torch.rand(hidden_size))\n",
    "        self.v.data.normal_(mean=0, std=1. / np.sqrt(self.v.size(0)))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: (batch_size, hidden_size)\n",
    "        # encoder_outputs: (seq_length, batch_size, hidden_size*2)\n",
    "        seq_length = encoder_outputs.shape[0]\n",
    "        hidden = hidden.repeat(seq_length, 1, 1).transpose(0, 1)  # (batch_size, seq_length, hidden_size)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # (batch_size, seq_length, hidden_size*2)\n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, seq_length, hidden_size)\n",
    "        attention = torch.softmax(torch.sum(self.v * energy, dim=2), dim=1).unsqueeze(1)  # (batch_size, 1, seq_length)\n",
    "        return attention\n",
    "    \n",
    "# Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = torch.nn.LSTM(embedding_size+hidden_size*2, hidden_size, num_layers, dropout=dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = torch.nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # x: (batch_size)\n",
    "        # hidden: (num_layers, batch_size, hidden_size)\n",
    "        # cell: (num_layers, batch_size, hidden_size)\n",
    "        # encoder_outputs: (seq_length, batch_size, hidden_size*2)\n",
    "        x = x.unsqueeze(0)  # (1, batch_size)\n",
    "        embedded = self.embedding(x)  # (1, batch_size, embedding_size)\n",
    "        attention = self.attention(hidden[-1], encoder_outputs)  # (batch_size, 1, seq_length)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # (batch_size, seq_length, hidden_size*2)\n",
    "        weighted = torch.bmm(attention, encoder_outputs)  # (batch_size, 1, hidden_size*2)\n",
    "        weighted = weighted.transpose(0, 1)  # (1, batch_size, hidden_size*2)\n",
    "        output, (hidden, cell) = self.lstm(torch.cat((embedded, weighted), dim=2), (hidden, cell))  # (1, batch_size, hidden_size)\n",
    "        prediction = self.fc(output.squeeze(0))  # (batch_size, input_size)\n",
    "        return prediction, hidden, cell # (batch_size, input_size), (num_layers, batch_size, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "    \n",
    "# Seq2Seq\n",
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        # source: (seq_length, batch_size)\n",
    "        # target: (seq_length, batch_size)\n",
    "        batch_size = source.shape[1]\n",
    "        seq_length = target.shape[0]\n",
    "        input_size = self.decoder.input_size\n",
    "        outputs = torch.zeros(seq_length, batch_size, input_size).to(self.device)  # (seq_length, batch_size, input_size)\n",
    "        encoder_outputs, hidden, cell = self.encoder(source)  # (seq_length, batch_size, hidden_size*2), (num_layers, batch_size, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        x = target[0]  # (batch_size)\n",
    "        for i in range(1, seq_length):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell, encoder_outputs)  # (batch_size, input_size), (num_layers, batch_size, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1)  # (batch_size)\n",
    "            x = target[i] if random.random() < teacher_forcing_ratio else best_guess\n",
    "        return outputs\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size_encoder = len(train_sources_vocabulary)\n",
    "input_size_decoder = len(train_targets_vocabulary)\n",
    "output_size = len(train_targets_vocabulary)\n",
    "encoder_embedding_size = 512\n",
    "decoder_embedding_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, num_layers, dropout).to(device)\n",
    "model = Seq2Seq(encoder_net, decoder_net, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# train\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        source = batch[0].to(device)\n",
    "        target = batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source, target)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        target = target[1:].view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "num_epochs = 5\n",
    "clip = 1\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, optimizer, criterion, clip)\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
